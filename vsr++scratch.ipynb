{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/snehith-3939/FirstRepo/blob/main/vsr%2B%2Bscratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchmetrics"
      ],
      "metadata": {
        "id": "wNxH66jxMbfA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74c025ba-4199-40ae-e285-e4766edc1d50",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.7.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.6.0+cu124)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.13.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
            "Downloading torchmetrics-1.7.1-py3-none-any.whl (961 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed lightning-utilities-0.14.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torchmetrics-1.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCP2xcNixtqd"
      },
      "source": [
        "#model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -qU"
      ],
      "metadata": {
        "id": "S9imtcP1iu4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Log in to your W&B account\n",
        "import wandb\n",
        "import random\n",
        "import math"
      ],
      "metadata": {
        "id": "yUfKl9tni0Rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "id": "2RVEN7hFi2_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.ops import DeformConv2d"
      ],
      "metadata": {
        "id": "jcA51JcGdjaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super().__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channels, channels // reduction),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(channels // reduction, channels),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y.expand_as(x)"
      ],
      "metadata": {
        "id": "tXGCzcg6HSIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels=64, reduction=16):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, in_channels, 3, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
        "        self.relu = nn.LeakyReLU(0.1, inplace=True)\n",
        "        self.conv2 = nn.Conv2d(in_channels, in_channels, 3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(in_channels)\n",
        "\n",
        "        # Channel Attention\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(in_channels, in_channels//reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(in_channels//reduction, in_channels, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "\n",
        "        # Channel attention\n",
        "        b, c, _, _ = out.size()\n",
        "        y = self.avg_pool(out).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        out = out * y.expand_as(out)\n",
        "\n",
        "        out += identity\n",
        "        return out"
      ],
      "metadata": {
        "id": "6PfM3jitdjWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FlowEstimation(nn.Module):\n",
        "    def __init__(self, num_feat=64, use_decoder=True):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(2*num_feat, num_feat, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_feat),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv2d(num_feat, num_feat, 3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_feat),\n",
        "            nn.LeakyReLU(0.1),\n",
        "        )\n",
        "\n",
        "        self.flow_pred = nn.Conv2d(num_feat, 2, 3, padding=1, bias=False)\n",
        "        self.use_decoder = use_decoder\n",
        "        if use_decoder:\n",
        "            self.decoder = nn.Sequential(\n",
        "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "                nn.Conv2d(2, 2, 3, padding=1),\n",
        "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "                nn.Conv2d(2, 2, 3, padding=1),\n",
        "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "                nn.Conv2d(2, 2, 3, padding=1),\n",
        "            )\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x = torch.cat([x1, x2], dim=1)\n",
        "        x = self.encoder(x)\n",
        "        flow = self.flow_pred(x)\n",
        "        if self.use_decoder:\n",
        "            flow = self.decoder(flow)\n",
        "        return flow"
      ],
      "metadata": {
        "id": "qISU0zxNdjQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BidirectionalPropagation(nn.Module):\n",
        "    def __init__(self, num_feat, num_block):\n",
        "        super().__init__()\n",
        "        self.deform_align = DeformableAlignment(num_feat)\n",
        "        self.fuse_conv = nn.Conv2d(2*num_feat, num_feat, 3, padding=1)\n",
        "        self.blocks = nn.Sequential(*[ResidualBlock(num_feat) for _ in range(num_block)])\n",
        "\n",
        "    def forward(self, feat, hidden, flow):\n",
        "        if hidden is None:\n",
        "            hidden = torch.zeros_like(feat)\n",
        "        aligned_hidden = self.deform_align(feat, hidden, flow)\n",
        "        fused = self.fuse_conv(torch.cat([feat, aligned_hidden], dim=1))\n",
        "        out = self.blocks(fused)\n",
        "        return out, out"
      ],
      "metadata": {
        "id": "x6d_AxK35nEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeformableAlignment(nn.Module):\n",
        "    def __init__(self, num_feat=64):\n",
        "        super().__init__()\n",
        "        self.offset_conv = nn.Sequential(\n",
        "            nn.Conv2d(num_feat*2 + 2, num_feat, 3, padding=1),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv2d(num_feat, num_feat, 3, padding=1),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv2d(num_feat, 27, 3, padding=1),\n",
        "        )\n",
        "        self.deform_conv = DeformConv2d(num_feat, num_feat, 3, padding=1)\n",
        "\n",
        "    def forward(self, feat, neighbor_feat, flow):\n",
        "        B, C, H, W = feat.shape\n",
        "\n",
        "        # Generate grid with proper dimensions\n",
        "        affine_matrix = torch.eye(2, 3, device=feat.device).unsqueeze(0).expand(B, -1, -1)\n",
        "        grid = F.affine_grid(affine_matrix, neighbor_feat.size(), align_corners=False)\n",
        "\n",
        "        # Upsample flow to match feature resolution\n",
        "        upsampled_flow = F.interpolate(flow, size=(H, W), mode='bilinear', align_corners=False)\n",
        "        warped_grid = grid + upsampled_flow.permute(0, 2, 3, 1)\n",
        "\n",
        "        # Warp neighbor_feat using the warped_grid\n",
        "        warped_feat = F.grid_sample(neighbor_feat, warped_grid, mode='bilinear', padding_mode='border', align_corners=False)\n",
        "\n",
        "        # Predict offsets and masks\n",
        "        offset_mask = self.offset_conv(torch.cat([feat, warped_feat, upsampled_flow], dim=1))\n",
        "        offset = offset_mask[:, :18, :, :]\n",
        "        mask = torch.sigmoid(offset_mask[:, 18:, :, :])\n",
        "\n",
        "        # Deformable convolution\n",
        "        aligned_feat = self.deform_conv(warped_feat, offset, mask)\n",
        "        return aligned_feat"
      ],
      "metadata": {
        "id": "Pw4CBkThdjMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicVSRPlusPlus(nn.Module):\n",
        "    def __init__(self, scale=4, num_feat=64, num_block=30):\n",
        "        super().__init__()\n",
        "        self.scale = scale\n",
        "\n",
        "        # Feature extraction\n",
        "        self.feat_extract = nn.Sequential(\n",
        "            nn.Conv2d(3, num_feat, 3, padding=1),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv2d(num_feat, num_feat, 3, padding=1),\n",
        "        )\n",
        "\n",
        "        # Propagation\n",
        "        self.forward_prop = BidirectionalPropagation(num_feat, num_block//2)\n",
        "        self.backward_prop = BidirectionalPropagation(num_feat, num_block//2)\n",
        "\n",
        "        # Flow estimation\n",
        "        self.flow_estimation = FlowEstimation(num_feat)\n",
        "\n",
        "        # Fusion and reconstruction (Added ChannelAttention)\n",
        "        self.fusion = nn.Conv2d(2*num_feat, num_feat, 3, padding=1)\n",
        "        self.reconstruction = nn.Sequential(\n",
        "            ResidualBlock(num_feat),\n",
        "            nn.Conv2d(num_feat, num_feat, 3, padding=1),\n",
        "            ChannelAttention(num_feat),  # Added\n",
        "            nn.Conv2d(num_feat, 3*(scale**2), 3, padding=1),\n",
        "            nn.PixelShuffle(scale),\n",
        "            nn.Conv2d(3, 3, 3, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, lr_seq):\n",
        "        B, T, C, H, W = lr_seq.shape\n",
        "\n",
        "        # Feature extraction\n",
        "        lr_feats = [self.feat_extract(lr_seq[:, t]) for t in range(T)]\n",
        "\n",
        "        # Compute flows\n",
        "        forward_flows, backward_flows = [], []\n",
        "        for t in range(T):\n",
        "            if t < T-1:\n",
        "                fwd_flow = self.flow_estimation(lr_feats[t], lr_feats[t+1])\n",
        "            else:\n",
        "                fwd_flow = torch.zeros_like(forward_flows[-1]) if forward_flows else torch.zeros(B,2,H,W, device=lr_seq.device)\n",
        "            forward_flows.append(fwd_flow)\n",
        "\n",
        "            if t > 0:\n",
        "                bwd_flow = self.flow_estimation(lr_feats[t], lr_feats[t-1])\n",
        "            else:\n",
        "                bwd_flow = torch.zeros_like(backward_flows[-1]) if backward_flows else torch.zeros(B,2,H,W, device=lr_seq.device)\n",
        "            backward_flows.append(bwd_flow)\n",
        "\n",
        "        # Forward propagation\n",
        "        forward_feats, hidden = [], None\n",
        "        for t in range(T):\n",
        "            feat, hidden = self.forward_prop(lr_feats[t], hidden, forward_flows[t])\n",
        "            forward_feats.append(feat)\n",
        "\n",
        "        # Backward propagation\n",
        "        backward_feats, hidden = [], None\n",
        "        for t in reversed(range(T)):\n",
        "            feat, hidden = self.backward_prop(lr_feats[t], hidden, backward_flows[t])\n",
        "            backward_feats.insert(0, feat)\n",
        "\n",
        "        # Fusion and reconstruction\n",
        "        sr_outputs = []\n",
        "        for t in range(T):\n",
        "            fused = self.fusion(torch.cat([forward_feats[t], backward_feats[t]], dim=1))\n",
        "            sr = self.reconstruction(fused)\n",
        "            sr_outputs.append(sr)\n",
        "\n",
        "        return torch.stack(sr_outputs, dim=1)"
      ],
      "metadata": {
        "id": "qQ0111Zf3yZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add at the top\n",
        "!pip install opencv-python-headless tqdm  # Required for image processing\n",
        "import cv2\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKmEd1DVPLdA",
        "outputId": "142617bf-17ac-4049-95ab-7260fd71c9ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python-headless) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import random\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import zipfile\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "vgOUOdo-HgMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#dataset preprocessing"
      ],
      "metadata": {
        "id": "vNJkGAAH6J9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gdown\n",
        "\n",
        "# Use gdown to download the ZIP file from shared Drive link\n",
        "file_id = \"17iJgEQHkoT9FQKgvarslGXvK_hcxmKD8\"\n",
        "!gdown --id {file_id} --output images.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlZwt31-_vuN",
        "outputId": "c6e2db11-9c92-4e9e-836f-ee1f611e2e04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=17iJgEQHkoT9FQKgvarslGXvK_hcxmKD8\n",
            "From (redirected): https://drive.google.com/uc?id=17iJgEQHkoT9FQKgvarslGXvK_hcxmKD8&confirm=t&uuid=50d2580b-56ea-41ee-b2c2-453bb30c5320\n",
            "To: /content/images.zip\n",
            "100% 9.82G/9.82G [03:55<00:00, 41.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def unzip_file(zip_filepath, extract_dir):\n",
        "  \"\"\"Unzips a file to a specified directory.\n",
        "\n",
        "  Args:\n",
        "    zip_filepath: Path to the zip file.\n",
        "    extract_dir: Directory to extract the contents to.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    with zipfile.ZipFile(zip_filepath, 'r') as zip_ref:\n",
        "      zip_ref.extractall(extract_dir)\n",
        "    print(f\"Successfully unzipped {zip_filepath} to {extract_dir}\")\n",
        "  except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {zip_filepath}\")\n",
        "  except zipfile.BadZipFile:\n",
        "    print(f\"Error: Invalid zip file at {zip_filepath}\")\n",
        "  except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "# Example usage (replace with your actual file paths):\n",
        "unzip_file(\"/content/images.zip\", \"/content/dataset\")\n"
      ],
      "metadata": {
        "id": "KtNEhT2w15u9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5403455-3de6-4338-9958-eaa03eb88d2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully unzipped /content/images.zip to /content/dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "root_folder = \"dataset\"  # Main folder with subfolders like '000', '001', etc.\n",
        "\n",
        "# Recursively collect all PNG/JPG images\n",
        "image_files = []\n",
        "for dirpath, _, filenames in os.walk(root_folder):\n",
        "    for file in filenames:\n",
        "        if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "            image_files.append(os.path.join(dirpath, file))\n",
        "\n",
        "# Gather info\n",
        "formats = {}\n",
        "resolutions = {}\n",
        "total_size_kb = 0\n",
        "\n",
        "for fpath in image_files:\n",
        "    try:\n",
        "        img = Image.open(fpath)\n",
        "        fmt = img.format\n",
        "        size = img.size\n",
        "\n",
        "        # Count format and resolution\n",
        "        formats[fmt] = formats.get(fmt, 0) + 1\n",
        "        resolutions[size] = resolutions.get(size, 0) + 1\n",
        "\n",
        "        # File size\n",
        "        total_size_kb += os.path.getsize(fpath) / 1024\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Could not open {fpath}: {e}\")\n",
        "\n",
        "# Print summary\n",
        "print(f\"\\nğŸ“ Total images found: {len(image_files)}\")\n",
        "print(f\"ğŸ§¾ Total dataset size: {total_size_kb:.2f} KB\")\n",
        "\n",
        "print(\"\\nğŸ–¼ï¸ Image formats:\")\n",
        "for fmt, count in formats.items():\n",
        "    print(f\" - {fmt}: {count} images\")\n",
        "\n",
        "print(\"\\nğŸ“ Image resolutions:\")\n",
        "for res, count in resolutions.items():\n",
        "    print(f\" - {res[0]}x{res[1]}: {count} images\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoT9qeDJysnm",
        "outputId": "0ff0cb38-731a-477c-d632-a2291a0779ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“ Total images found: 24000\n",
            "ğŸ§¾ Total dataset size: 9590304.56 KB\n",
            "\n",
            "ğŸ–¼ï¸ Image formats:\n",
            " - PNG: 24000 images\n",
            "\n",
            "ğŸ“ Image resolutions:\n",
            " - 640x360: 24000 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "# Source and destination folders\n",
        "original_root = \"dataset\"\n",
        "output_root = \"main_dataset/hr_dataset\"\n",
        "\n",
        "# Get all folders (each folder = one video)\n",
        "video_folders = sorted([f for f in os.listdir(original_root) if os.path.isdir(os.path.join(original_root, f))])\n",
        "\n",
        "# Select half randomly\n",
        "random.seed(42)\n",
        "half_folders = random.sample(video_folders, len(video_folders) // 2)\n",
        "\n",
        "# Create output folder\n",
        "os.makedirs(output_root, exist_ok=True)\n",
        "\n",
        "for folder in half_folders:\n",
        "    src_folder = os.path.join(original_root, folder)\n",
        "    dst_folder = os.path.join(output_root, folder)\n",
        "    os.makedirs(dst_folder, exist_ok=True)\n",
        "\n",
        "    for filename in os.listdir(src_folder):\n",
        "        src_path = os.path.join(src_folder, filename)\n",
        "\n",
        "        if filename.lower().endswith(\".png\"):\n",
        "            # Convert PNG â†’ JPEG (high quality)\n",
        "            dst_filename = filename.rsplit('.', 1)[0] + \".jpg\"\n",
        "            dst_path = os.path.join(dst_folder, dst_filename)\n",
        "\n",
        "            try:\n",
        "                img = Image.open(src_path).convert(\"RGB\")\n",
        "                img.save(dst_path, \"JPEG\", quality=95)  # High quality, minimal compression\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ Failed to convert {src_path}: {e}\")\n",
        "\n",
        "        elif filename.lower().endswith((\".jpg\", \".jpeg\")):\n",
        "            # Copy existing JPEGs as-is\n",
        "            dst_path = os.path.join(dst_folder, filename)\n",
        "            shutil.copy2(src_path, dst_path)\n",
        "\n",
        "print(f\"âœ… Copied and converted {len(half_folders)} folders to '{output_root}' (PNG â†’ JPEG, no compression)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_e3iy1bJdQq",
        "outputId": "a562c2c4-921d-4306-e174-7b61f21db59c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Copied and converted 120 folders to 'main_dataset/hr_dataset' (PNG â†’ JPEG, no compression)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Set your dataset path\n",
        "root_folder = \"main_dataset/hr_dataset\"\n",
        "\n",
        "# Collect all image file paths\n",
        "image_files = []\n",
        "for dirpath, _, filenames in os.walk(root_folder):\n",
        "    for file in filenames:\n",
        "        if file.lower().endswith(('.jpg', '.jpeg')):\n",
        "            image_files.append(os.path.join(dirpath, file))\n",
        "\n",
        "# Analyze dataset\n",
        "formats = {}\n",
        "resolutions = {}\n",
        "total_size_kb = 0\n",
        "\n",
        "for fpath in image_files:\n",
        "    try:\n",
        "        img = Image.open(fpath)\n",
        "        fmt = img.format\n",
        "        size = img.size\n",
        "\n",
        "        formats[fmt] = formats.get(fmt, 0) + 1\n",
        "        resolutions[size] = resolutions.get(size, 0) + 1\n",
        "        total_size_kb += os.path.getsize(fpath) / 1024\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Could not open {fpath}: {e}\")\n",
        "\n",
        "# Print result like your example\n",
        "print(f\"ğŸ“ Total images found: {len(image_files)}\")\n",
        "print(f\"ğŸ§¾ Total dataset size: {total_size_kb:.2f} KB\\n\")\n",
        "\n",
        "print(\"ğŸ–¼ï¸ Image formats:\")\n",
        "for fmt, count in formats.items():\n",
        "    print(f\" - {fmt}: {count} images\")\n",
        "\n",
        "print(\"\\nğŸ“ Image resolutions:\")\n",
        "for res, count in resolutions.items():\n",
        "    print(f\" - {res[0]}x{res[1]}: {count} images\")\n"
      ],
      "metadata": {
        "id": "QtB7avazLT8B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67c335dc-23af-4824-a631-c11a40e671de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“ Total images found: 12000\n",
            "ğŸ§¾ Total dataset size: 1312686.03 KB\n",
            "\n",
            "ğŸ–¼ï¸ Image formats:\n",
            " - JPEG: 12000 images\n",
            "\n",
            "ğŸ“ Image resolutions:\n",
            " - 640x360: 12000 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# Dataset path\n",
        "dataset_path = \"half_videos_png2jpeg\"\n",
        "\n",
        "# Get all image paths\n",
        "image_paths = []\n",
        "for dirpath, _, filenames in os.walk(dataset_path):\n",
        "    for file in filenames:\n",
        "        if file.lower().endswith(('.jpg', '.jpeg')):\n",
        "            image_paths.append(os.path.join(dirpath, file))"
      ],
      "metadata": {
        "id": "fGUMMDr3LjVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "def generate_lr_images(hr_dir, lr_dir, scale):\n",
        "    for root, _, files in os.walk(hr_dir):\n",
        "        for fname in tqdm(files):\n",
        "            hr_path = os.path.join(root, fname)\n",
        "\n",
        "            try:\n",
        "                hr_img = Image.open(hr_path).convert(\"RGB\")\n",
        "                w, h = hr_img.size\n",
        "                lr_img = hr_img.resize((w // scale, h // scale), Image.BICUBIC)\n",
        "\n",
        "                # Maintain directory structure\n",
        "                rel_path = os.path.relpath(hr_path, hr_dir)\n",
        "                lr_path = os.path.join(lr_dir, rel_path)\n",
        "                os.makedirs(os.path.dirname(lr_path), exist_ok=True)\n",
        "\n",
        "                lr_img.save(lr_path)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {fname}: {e}\")"
      ],
      "metadata": {
        "id": "pDEY0Rc9CXYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate_lr_images(\"/content/main_dataset/hr_dataset\", \"/content/main_dataset/lr_dataset\", scale=4)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3cx8QS_c9jlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_pairs(hr_root, lr_root, scale):\n",
        "    for seq in tqdm(os.listdir(hr_root)):\n",
        "        hr_seq = os.path.join(hr_root, seq)\n",
        "        lr_seq = os.path.join(lr_root, seq)\n",
        "\n",
        "        for frame in os.listdir(hr_seq):\n",
        "            hr_img = Image.open(os.path.join(hr_seq, frame))\n",
        "            lr_img = Image.open(os.path.join(lr_seq, frame))\n",
        "\n",
        "            # Validate scale factor\n",
        "            assert hr_img.width == lr_img.width * scale\n",
        "            assert hr_img.height == lr_img.height * scale\n",
        "\n",
        "validate_pairs(\"/content/train_hr\", \"/content/train_lr\", 4)\n",
        "validate_pairs(\"/content/val_hr\", \"/content/val_lr\", 4)"
      ],
      "metadata": {
        "id": "5bp0cfvGOPa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#load dataset"
      ],
      "metadata": {
        "id": "MRvfiJGjnqRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class REDSDataset(Dataset):\n",
        "    def __init__(self, hr_root, lr_root, scale=4, train=True, seq_length=5, crop_size=256):\n",
        "        self.hr_root = hr_root\n",
        "        self.lr_root = lr_root\n",
        "        self.scale = scale\n",
        "        self.train = train\n",
        "        self.seq_length = seq_length\n",
        "        self.crop_size = crop_size\n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "\n",
        "        # Collect sequences\n",
        "        self.sequences = []\n",
        "        for seq in os.listdir(hr_root):\n",
        "            seq_dir = os.path.join(hr_root, seq)\n",
        "            frames = sorted([f for f in os.listdir(seq_dir) if f.endswith('.jpg')])\n",
        "            # Ensure enough frames for the sequence length\n",
        "            for start in range(0, len(frames) - seq_length + 1):\n",
        "                self.sequences.append((seq, start, start + seq_length))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq, start, end = self.sequences[idx]\n",
        "        hr_imgs = []\n",
        "        lr_imgs = []\n",
        "\n",
        "        # Determine augmentations once per sequence\n",
        "        if self.train:\n",
        "            reverse = random.random() > 0.5\n",
        "            flip = random.random() > 0.5\n",
        "            # Get crop coordinates from the first frame\n",
        "            sample_hr_path = os.path.join(self.hr_root, seq, f\"{start:08d}.jpg\")\n",
        "            with Image.open(sample_hr_path) as sample_hr:\n",
        "                w, h = sample_hr.size\n",
        "            x = random.randint(0, w - self.crop_size)\n",
        "            y = random.randint(0, h - self.crop_size)\n",
        "        else:\n",
        "            reverse = False\n",
        "            flip = False\n",
        "            # Center crop for validation if needed (example)\n",
        "            sample_hr_path = os.path.join(self.hr_root, seq, f\"{start:08d}.jpg\")\n",
        "            with Image.open(sample_hr_path) as sample_hr:\n",
        "                w, h = sample_hr.size\n",
        "            x = (w - self.crop_size) // 2\n",
        "            y = (h - self.crop_size) // 2\n",
        "\n",
        "        frame_indices = list(range(start, end))\n",
        "        if reverse:\n",
        "            frame_indices = reversed(frame_indices)\n",
        "\n",
        "        for frame_idx in frame_indices:\n",
        "            frame_num = f\"{frame_idx:08d}\"\n",
        "            hr_path = os.path.join(self.hr_root, seq, f\"{frame_num}.jpg\")\n",
        "            lr_path = os.path.join(self.lr_root, seq, f\"{frame_num}.jpg\")\n",
        "\n",
        "            hr_img = Image.open(hr_path).convert('RGB')\n",
        "            lr_img = Image.open(lr_path).convert('RGB')\n",
        "\n",
        "            # Apply crop\n",
        "            hr_img = hr_img.crop((x, y, x + self.crop_size, y + self.crop_size))\n",
        "            lr_crop_size = self.crop_size // self.scale\n",
        "            lr_x = x // self.scale\n",
        "            lr_y = y // self.scale\n",
        "            lr_img = lr_img.crop((lr_x, lr_y, lr_x + lr_crop_size, lr_y + lr_crop_size))\n",
        "\n",
        "            # Apply flip\n",
        "            if flip:\n",
        "                hr_img = hr_img.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "                lr_img = lr_img.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "\n",
        "            hr_imgs.append(hr_img)\n",
        "            lr_imgs.append(lr_img)\n",
        "\n",
        "        # Convert to tensors\n",
        "        lr_seq = torch.stack([self.to_tensor(img) for img in lr_imgs])\n",
        "        hr_seq = torch.stack([self.to_tensor(img) for img in hr_imgs])\n",
        "\n",
        "        return lr_seq, hr_seq"
      ],
      "metadata": {
        "id": "9qSAIFOoCjAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = REDSDataset(hr_root=\"/content/main_dataset/hr_dataset\", lr_root=\"/content/main_dataset/lr_dataset\", seq_length=5)\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [int(0.8 * len(dataset)), len(dataset) - int(0.8 * len(dataset))], random.seed(42))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=24, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=24, shuffle=False)"
      ],
      "metadata": {
        "id": "GqTHd1Y2pOk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#train the model"
      ],
      "metadata": {
        "id": "Q6IaXYMQCdf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "id": "LBQEC6DhC2hG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9c5e32fe-8108-45ad-f472-95b335c3cd12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wandb"
      ],
      "metadata": {
        "id": "rPNtbUBmCjqq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dd4d0b4-3166-4588-e956-3a9a20f969f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.9)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.25.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.1)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4wqHtadNCOu",
        "outputId": "84a0dde8-e777-4f96-87b7-610685a86e13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maditya5748rai\u001b[0m (\u001b[33maditya5748rai-iit-indore\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from torchmetrics.image import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train(train_loader, val_loader):\n",
        "    print('Initializing training...')\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Configuration\n",
        "    config = {\n",
        "        \"scale\": 4,\n",
        "        \"num_epochs\": 18,\n",
        "        \"lr\": 0.02,\n",
        "        \"weight_decay\": 0.0005,\n",
        "        \"architecture\": \"BasicVSRPlusPlus\",\n",
        "        \"dataset\": \"REDS\",\n",
        "        \"optimizer\": \"AdamW\",\n",
        "        \"loss\": \"L1Loss\"\n",
        "    }\n",
        "\n",
        "    # Initialize wandb\n",
        "    wandb.init(\n",
        "        project=\"video-super-resolution\",\n",
        "        config=config,\n",
        "        notes=\"Training BasicVSR++ \"+config[\"dataset\"]+\" with scale factor \"+str(config[\"scale\"])\n",
        "    )\n",
        "\n",
        "    # Model initialization\n",
        "    model = BasicVSRPlusPlus(scale=config[\"scale\"]).to(device, memory_format=torch.channels_last)\n",
        "    wandb.watch(model, log=\"gradients\", log_freq=100)  # Log model topology and gradients\n",
        "\n",
        "    # Loss and metrics\n",
        "    criterion = nn.L1Loss()\n",
        "    psnr_metric = PeakSignalNoiseRatio().to(device)\n",
        "    ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
        "\n",
        "    # Optimizer and scheduler\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config[\"num_epochs\"])\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    best_psnr = 0.0\n",
        "    print('Starting training loop...')\n",
        "\n",
        "    for epoch in range(config[\"num_epochs\"]):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['num_epochs']} [Train]\")\n",
        "\n",
        "        for batch_idx, (lr_seq, hr_seq) in enumerate(progress_bar):\n",
        "            # Data preparation\n",
        "            lr_seq = lr_seq.to(device, non_blocking=True, memory_format=torch.channels_last_3d)\n",
        "            hr_seq = hr_seq.to(device, non_blocking=True, memory_format=torch.channels_last_3d)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            # Mixed precision training\n",
        "            with autocast():\n",
        "                sr_seq = model(lr_seq)\n",
        "                loss = criterion(sr_seq, hr_seq)\n",
        "\n",
        "            # Gradient management\n",
        "            scaler.scale(loss).backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.01)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            # Logging\n",
        "            train_loss += loss.item()\n",
        "            progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_psnr, val_ssim = 0.0, 0.0\n",
        "        sample_images = []\n",
        "        with torch.no_grad():\n",
        "            val_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{config['num_epochs']} [Val]\")\n",
        "            for lr_val, hr_val in val_bar:\n",
        "                lr_val = lr_val.to(device, non_blocking=True, memory_format=torch.channels_last_3d)\n",
        "                hr_val = hr_val.to(device, non_blocking=True, memory_format=torch.channels_last_3d)\n",
        "\n",
        "                with autocast():\n",
        "                    sr_val = model(lr_val)\n",
        "\n",
        "                # Metrics calculation\n",
        "                val_psnr += psnr_metric(sr_val, hr_val)\n",
        "                # val_ssim += ssim_metric(sr_val, hr_val)\n",
        "\n",
        "                # Collect sample images\n",
        "                if not sample_images and epoch % 2 == 0:  # Log every 2 epochs\n",
        "                    idx = 0  # First sample in batch\n",
        "                    t = sr_val.shape[1] // 2  # Middle frame\n",
        "                    sample_images.extend([\n",
        "                        wandb.Image(sr_val[idx, t].cpu().permute(1, 2, 0).numpy()),\n",
        "                        wandb.Image(hr_val[idx, t].cpu().permute(1, 2, 0).numpy()),\n",
        "                        wandb.Image(lr_val[idx, t].cpu().permute(1, 2, 0).numpy())])\n",
        "\n",
        "        # Calculate averages\n",
        "        avg_loss = train_loss / len(train_loader)\n",
        "        avg_psnr = val_psnr / len(val_loader)\n",
        "        # avg_ssim = val_ssim / len(val_loader)\n",
        "\n",
        "        # Wandb logging\n",
        "        log_data = {\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": avg_loss,\n",
        "            \"val_psnr\": avg_psnr,\n",
        "            # \"val_ssim\": avg_ssim,\n",
        "            \"learning_rate\": scheduler.get_last_lr()[0]\n",
        "        }\n",
        "\n",
        "        if sample_images:\n",
        "            log_data.update({\n",
        "                \"SR Sample\": sample_images[0],\n",
        "                \"HR Ground Truth\": sample_images[1],\n",
        "                \"LR Input\": sample_images[2]\n",
        "            })\n",
        "\n",
        "        wandb.log(log_data)\n",
        "\n",
        "        # Update scheduler and save best model\n",
        "        scheduler.step()\n",
        "        if avg_psnr > best_psnr:\n",
        "            best_psnr = avg_psnr\n",
        "            torch.save(model.state_dict(), 'best_reds_vsr.pth')\n",
        "            wandb.save('best_reds_vsr.pth')\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{config['num_epochs']} | \"\n",
        "              f\"Train Loss: {avg_loss:.4f} | \"\n",
        "              f\"Val PSNR: {avg_psnr:.2f} | \"\n",
        "            #   f\"Val SSIM: {avg_ssim:.4f}\")\n",
        "\n",
        "    wandb.finish()\n",
        "    print(\"Training completed!\")"
      ],
      "metadata": {
        "id": "VYubtu0P9mn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(train_loader=train_loader, val_loader=val_loader)"
      ],
      "metadata": {
        "id": "ypyjIpL69sbx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "88efd51e-2a37-4666-beae-c0a029288322"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">devoted-water-1</strong> at: <a href='https://wandb.ai/aditya5748rai-iit-indore/video-super-resolution/runs/srliyd7z' target=\"_blank\">https://wandb.ai/aditya5748rai-iit-indore/video-super-resolution/runs/srliyd7z</a><br> View project at: <a href='https://wandb.ai/aditya5748rai-iit-indore/video-super-resolution' target=\"_blank\">https://wandb.ai/aditya5748rai-iit-indore/video-super-resolution</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250414_113148-srliyd7z/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250414_114308-zfb4xgm2</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aditya5748rai-iit-indore/video-super-resolution/runs/zfb4xgm2' target=\"_blank\">driven-music-2</a></strong> to <a href='https://wandb.ai/aditya5748rai-iit-indore/video-super-resolution' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/aditya5748rai-iit-indore/video-super-resolution' target=\"_blank\">https://wandb.ai/aditya5748rai-iit-indore/video-super-resolution</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/aditya5748rai-iit-indore/video-super-resolution/runs/zfb4xgm2' target=\"_blank\">https://wandb.ai/aditya5748rai-iit-indore/video-super-resolution/runs/zfb4xgm2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-58-d421b165d13d>:44: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training loop...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/18 [Train]:   0%|          | 0/384 [00:00<?, ?it/s]<ipython-input-58-d421b165d13d>:62: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Epoch 1/18 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 384/384 [10:32<00:00,  1.65s/it, loss=0.0673]\n",
            "Epoch 1/18 [Val]:   0%|          | 0/96 [00:00<?, ?it/s]<ipython-input-58-d421b165d13d>:86: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Epoch 1/18 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96/96 [01:38<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'avg_ssim' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-3a173ead7cfd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-58-d421b165d13d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, val_loader)\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;34m\"train_loss\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mavg_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;34m\"val_psnr\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mavg_psnr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;34m\"val_ssim\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mavg_ssim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0;34m\"learning_rate\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_last_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         }\n",
            "\u001b[0;31mNameError\u001b[0m: name 'avg_ssim' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6LBCGcDyEc7a"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "vNJkGAAH6J9-"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}